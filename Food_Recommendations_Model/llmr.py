# -*- coding: utf-8 -*-
"""LLMR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FhbI7WgYB2-DttB3ldcVJ9kQYQDZ3aHH

# Here I will start importing the pre-trained model using BERT
"""

from transformers import BertTokenizer, BertForSequenceClassification

# Load pre-trained BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

# Tokenize input text
inputs = tokenizer("This product is amazing!", return_tensors="pt")

# Perform sentiment classification
outputs = model(**inputs)
logits = outputs.logits

pip install torch transformers datasets tqdm

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load tokenizer and pre-trained model
model_name = "gpt2"  # Replace with the model you want
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

"""# After that step is complete I will start uploading my datasets for my food receomedation model. These datasets came from Yelp.com"""

import pandas as pd

# Specify the paths to your JSON files
business_file = '/content/drive/MyDrive/Fed_res_input/kag/yelp_academic_dataset_business.json'
checkin_file = '/content/drive/MyDrive/Fed_res_input/kag/yelp_academic_dataset_checkin.json'
review_file = '/content/drive/MyDrive/Fed_res_input/kag/yelp_academic_dataset_review.json'
tip_file = '/content/drive/MyDrive/Fed_res_input/kag/yelp_academic_dataset_tip.json'
user_file = '/content/drive/MyDrive/Fed_res_input/kag/yelp_academic_dataset_user.json'

# Load each file one by one
print("Loading business dataset...")
business_df = pd.read_json(business_file, lines=True)
print(f"Business dataset loaded with shape: {business_df.shape}")

print("Loading checkin dataset...")
checkin_df = pd.read_json(checkin_file, lines=True)
print(f"Checkin dataset loaded with shape: {checkin_df.shape}")

print("Loading review dataset...")
review_df = pd.read_json(review_file, lines=True)
print(f"Review dataset loaded with shape: {review_df.shape}")

print("Loading tip dataset...")
tip_df = pd.read_json(tip_file, lines=True)
print(f"Tip dataset loaded with shape: {tip_df.shape}")

print("Loading user dataset...")
user_df = pd.read_json(user_file, lines=True)
print(f"User dataset loaded with shape: {user_df.shape}")

"""# The Reviews file was massive so i needed to upload thius in chunks

## In my training i dealt with relatively small data so I now know how to handle large data when i come across it
"""

import pandas as pd


review_file = '/content/drive/MyDrive/Fed_res_input/kag/yelp_academic_dataset_review.json'

chunk_size = 10000  # Adjust based on your RAM capacity
chunks = []

print("Loading review dataset in chunks...")
for chunk in pd.read_json(review_file, lines=True, chunksize=chunk_size):
    chunks.append(chunk)
    print(f"Loaded chunk with shape: {chunk.shape}")

# Concatenate all chunks if needed
review_df = pd.concat(chunks, ignore_index=True)
print(f"Final review dataset shape: {review_df.shape}")

output_file = '/content/drive/MyDrive/Fed_res_input/kag/review_dataset.csv'
review_df.to_csv(output_file, index=False)
print(f"Review dataset saved to {output_file}")

"""# Once everything is done I can now print the data to see how the first couple lines look"""

# Display first few rows
print(review_df.head())

# Check basic information about the dataset
print(review_df.info())

# Check for missing values
print(review_df.isnull().sum())

# Summary statistics
print(review_df.describe())

# Remove rows with negative values in `useful`, `funny`, `cool`
review_df = review_df[(review_df['useful'] >= 0) & (review_df['funny'] >= 0) & (review_df['cool'] >= 0)]
print(f"Dataset shape after removing invalid rows: {review_df.shape}")

"""From here on until the next comments I did alot of trial and error due to small issues and basically learning on the fly as i came across problems. Readers please skip until the next comment"""

import pandas as pd

base_path = '/content/drive/MyDrive/Fed_res_input/kag/'

# Define the file names
files = {
    "business": "yelp_academic_dataset_business.json",
    "checkin": "yelp_academic_dataset_checkin.json",
    "tip": "yelp_academic_dataset_tip.json",
    "user": "yelp_academic_dataset_user.json"
}

# Load each dataset
datasets = {}
for name, file in files.items():
    print(f"Loading {name} dataset...")
    datasets[name] = pd.read_json(base_path + file, lines=True)
    print(f"{name.capitalize()} dataset shape: {datasets[name].shape}")
    print(datasets[name].info())
    print(datasets[name].head())
    print("\n")

import pandas as pd

# File paths
business_file = '/content/drive/MyDrive/Fed_res_input/kag/yelp_academic_dataset_business.json'
review_file = '/content/drive/MyDrive/Fed_res_input/kag/yelp_academic_dataset_review.json'
checkin_file = '/content/drive/MyDrive/Fed_res_input/kag/yelp_academic_dataset_checkin.json'
tip_file = '/content/drive/MyDrive/Fed_res_input/kag/yelp_academic_dataset_tip.json'
user_file = '/content/drive/MyDrive/Fed_res_input/kag/yelp_academic_dataset_user.json'

# Load datasets
print("Loading business dataset...")
business_df = pd.read_json(business_file, lines=True)
print(f"Business dataset shape: {business_df.shape}")

print("Loading review dataset in chunks...")
chunk_size = 10000  # Adjust chunk size to fit your memory capacity
chunks = []
for chunk in pd.read_json(review_file, lines=True, chunksize=chunk_size):
    chunks.append(chunk)
review_df = pd.concat(chunks, ignore_index=True)
print(f"Review dataset shape: {review_df.shape}")

print("Loading checkin dataset...")
checkin_df = pd.read_json(checkin_file, lines=True)
print(f"Checkin dataset shape: {checkin_df.shape}")

print("Loading tip dataset...")
tip_df = pd.read_json(tip_file, lines=True)
print(f"Tip dataset shape: {tip_df.shape}")

print("Loading user dataset...")
user_df = pd.read_json(user_file, lines=True)
print(f"User dataset shape: {user_df.shape}")

# Merge datasets (as an example, joining on business_id)
print("Merging datasets...")
merged_df = business_df.merge(review_df, on="business_id", how="left") \
    .merge(checkin_df, on="business_id", how="left") \
    .merge(tip_df, on="business_id", how="left") \
    .merge(user_df, left_on="business_id", right_on="user_id", how="left")

# Handle missing values by checking column data types
print("Handling missing values...")
for col in merged_df.columns:
    if merged_df[col].dtype == 'float64':  # For numeric columns
        merged_df[col].fillna(0, inplace=True)  # Replace NaN with 0
    elif merged_df[col].dtype == 'object':  # For string columns
        merged_df[col].fillna("", inplace=True)  # Replace NaN with empty string
    elif pd.api.types.is_datetime64_any_dtype(merged_df[col]):  # For datetime columns
        merged_df[col].fillna(pd.Timestamp.min, inplace=True)  # Replace NaT with a default date

print(f"Merged dataset shape: {merged_df.shape}")

# Save the merged dataframe to a CSV file
output_file = '/content/drive/MyDrive/Fed_res_input/kag/merged_yelp_dataset.csv'
merged_df.to_csv(output_file, index=False)
print(f"Merged dataset saved to {output_file}")

import tensorflow as tf

try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print(f"Running on TPU: {tpu.master()}")
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.TPUStrategy(tpu)
except ValueError:
    print("TPU not found. Make sure TPU is enabled in Colab.")

import pandas as pd

# Define file paths
base_path = '/content/drive/MyDrive/Fed_res_input/kag'

file_paths = {
    "business": f"{base_path}/yelp_academic_dataset_business.json",
    "review": f"{base_path}/yelp_academic_dataset_review.json",
    "checkin": f"{base_path}/yelp_academic_dataset_checkin.json",
    "tip": f"{base_path}/yelp_academic_dataset_tip.json",
    "user": f"{base_path}/yelp_academic_dataset_user.json"
}

# Load datasets
business_df = pd.read_json(file_paths['business'], lines=True)
checkin_df = pd.read_json(file_paths['checkin'], lines=True)
tip_df = pd.read_json(file_paths['tip'], lines=True)
user_df = pd.read_json(file_paths['user'], lines=True)

# Load the review dataset in chunks
chunk_size = 10000  # Adjust based on RAM
review_chunks = []
for chunk in pd.read_json(file_paths['review'], lines=True, chunksize=chunk_size):
    review_chunks.append(chunk)

review_df = pd.concat(review_chunks, ignore_index=True)

# Merge business with reviews
merged_df = review_df.merge(business_df, on='business_id', how='left')

# Merge with user data
merged_df = merged_df.merge(user_df, on='user_id', how='left')

# Merge with checkin data
merged_df = merged_df.merge(checkin_df, on='business_id', how='left')

# Merge with tip data
merged_df = merged_df.merge(tip_df, on=['business_id', 'user_id'], how='left')

# Fill missing values with empty strings or zeros
merged_df.fillna({'text': '', 'attributes': '', 'categories': '', 'hours': ''}, inplace=True)

# Filter necessary columns
filtered_df = merged_df[['text_x', 'categories', 'stars_x']].dropna()
filtered_df.rename(columns={'text_x': 'review_text', 'stars_x': 'stars'}, inplace=True)

print("Data successfully filtered!")
print(f"Filtered dataset shape: {filtered_df.shape}")

"""afdter we merged the data sets lets construct how are model should look"""

print(merged_df.columns)

# Check a sample of the review text
print(merged_df['text_x'].head())

# import torch
# from torch.utils.data import DataLoader, Dataset
# from transformers import AutoTokenizer, AutoModelForSequenceClassification
# from sklearn.preprocessing import LabelEncoder



# # Load tokenizer
# tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# # Prepare Label Encoder
# label_encoder = LabelEncoder()
# df['category_encoded'] = label_encoder.fit_transform(df['categories'])

# # Define Dataset for tokenized data
# class YelpDataset(Dataset):
#     def __init__(self, encodings, labels):
#         self.encodings = encodings
#         self.labels = labels

#     def __len__(self):
#         return len(self.labels)

#     def __getitem__(self, idx):
#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
#         item['labels'] = torch.tensor(self.labels[idx])
#         return item

# # Function to process chunks
# def process_chunk(chunk, batch_size):
#     # Tokenize chunk
#     encodings = tokenizer(list(chunk['text_x']), truncation=True, padding=True, max_length=128)
#     labels = chunk['category_encoded'].tolist()
#     dataset = YelpDataset(encodings, labels)
#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
#     return dataloader

# # Define model and optimizer
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=len(label_encoder.classes_))
# model.to(device)
# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# # Chunk training loop
# batch_size = 16
# epochs = 3
# chunk_size = 10000

# for epoch in range(epochs):
#     print(f"Epoch {epoch + 1}/{epochs}")
#     total_loss = 0
#     for chunk_start in range(0, len(df), chunk_size):
#         # Extract chunk
#         chunk_end = min(chunk_start + chunk_size, len(df))
#         chunk = df.iloc[chunk_start:chunk_end]

#         # Process chunk
#         train_loader = process_chunk(chunk, batch_size)

#         # Training step for each batch
#         model.train()
#         for batch in train_loader:
#             optimizer.zero_grad()
#             inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}
#             labels = batch['labels'].to(device)
#             outputs = model(**inputs, labels=labels)
#             loss = outputs.loss
#             total_loss += loss.item()
#             loss.backward()
#             optimizer.step()

#         print(f"Processed chunk {chunk_start}-{chunk_end} with loss: {total_loss / len(train_loader)}")
#     print(f"Epoch {epoch + 1} complete, Average Loss: {total_loss}")

# # Save the trained model
# model.save_pretrained("yelp_recommendation_model_chunked")
# tokenizer.save_pretrained("yelp_recommendation_model_chunked")

# print("Model saved!")



"""So the code commented out worked but used way too much resources even with my colab pro+ and there was just a better way to do this iteration so i began rewritting the code in a more efficient way.

Lets get more efficient. Again I will be doing more trials and errors and used pickle to save. Please skip until next comment
"""



import multiprocessing as mp
from transformers import BertTokenizer
from torch.utils.data import DataLoader, Dataset
import torch
import time
import psutil
import GPUtil

# Initialize GPU-accelerated fast tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased", use_fast=True)

# Function to log system resource usage
def log_resources():
    gpu_info = GPUtil.getGPUs()
    for gpu in gpu_info:
        print(f"GPU Memory Used: {gpu.memoryUsed}MB / {gpu.memoryTotal}MB")
    print(f"CPU RAM Usage: {psutil.virtual_memory().used / 1e9:.2f} GB")

# Function to tokenize a chunk of text with timing
def tokenize_chunk_with_timing(chunk_texts):
    start_time = time.time()
    tokenized = tokenizer(list(chunk_texts), truncation=True, padding=True, max_length=128)
    end_time = time.time()
    print(f"Processed chunk of size {len(chunk_texts)} in {end_time - start_time:.2f} seconds")
    log_resources()  # Log resource usage after processing
    return tokenized

# Split dataset into chunks for parallel processing
def parallel_tokenization(train_texts, num_processes=mp.cpu_count()):
    print(f"Tokenizing with {num_processes} processes...")
    chunk_size = len(train_texts) // num_processes
    chunks = [train_texts[i:i + chunk_size] for i in range(0, len(train_texts), chunk_size)]

    # Use multiprocessing to tokenize chunks in parallel
    with mp.Pool(processes=num_processes) as pool:
        tokenized_chunks = pool.map(tokenize_chunk_with_timing, chunks)

    # Combine all tokenized chunks into one dictionary
    final_tokenized_texts = {key: [] for key in tokenized_chunks[0]}
    for chunk in tokenized_chunks:
        for key, val in chunk.items():
            final_tokenized_texts[key].extend(val)

    return final_tokenized_texts

# Dataset class
class YelpDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

# Tokenization process
print("Preparing data for tokenization...")

# Parallel tokenization with bottleneck detection
start_time = time.time()
train_encodings = parallel_tokenization(train_texts)
end_time = time.time()
print(f"Total tokenization time: {end_time - start_time:.2f} seconds")

# Tokenize test set sequentially (small size)
test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128)

# Create datasets
train_dataset = YelpDataset(train_encodings, train_labels.tolist())
test_dataset = YelpDataset(test_encodings, test_labels.tolist())

# Create DataLoaders
batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

print("Tokenization complete!")

# Example: Split by category
subsets = {category: df[df['categories'] == category] for category in df['categories'].unique()}

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Filter and rename relevant columns for clarity
df = merged_df[['text_x', 'categories', 'stars_x']].dropna()
df.rename(columns={'text_x': 'review_text', 'stars_x': 'stars'}, inplace=True)

# Encode 'categories' into numerical labels
label_encoder = LabelEncoder()
df['category_encoded'] = label_encoder.fit_transform(df['categories'])

# Split the dataset into training and testing sets
train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['review_text'], df['category_encoded'], test_size=0.2, random_state=42
)

# Reset indices to ensure continuity in indexing
train_texts = train_texts.reset_index(drop=True)
test_texts = test_texts.reset_index(drop=True)
train_labels = train_labels.reset_index(drop=True)
test_labels = test_labels.reset_index(drop=True)

print("Data split and prepared:")
print(f"Train Texts: {train_texts.shape}")
print(f"Test Texts: {test_texts.shape}")
print(f"Train Labels: {train_labels.shape}")
print(f"Test Labels: {test_labels.shape}")



from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Select relevant columns
filtered_df = merged_df[['text_x', 'categories', 'stars_x']].dropna()

# Rename columns for clarity
filtered_df.rename(columns={'text_x': 'review_text', 'stars_x': 'stars'}, inplace=True)

# Encode 'categories' into numerical labels
label_encoder = LabelEncoder()
filtered_df['category_encoded'] = label_encoder.fit_transform(filtered_df['categories'])

# Split into train/test sets
train_texts, test_texts, train_labels, test_labels = train_test_split(
    filtered_df['review_text'], filtered_df['category_encoded'], test_size=0.2, random_state=42
)

print("Data preparation complete:")
print(f"Train texts: {len(train_texts)}, Test texts: {len(test_texts)}")

"""Now here i will use pickle to import tokenzied data, initialize the model to start training since this time it will be more efficient. I will stop the code after 2/3 is cokmplate due to resource save the model and run the rest and that will allow me to finish training the model

New?
"""

import pickle

# Path to your saved tokenized data
tokenized_data_path = "/content/drive/MyDrive/final_tokenized_data.pkl"

# Load the tokenized data
with open(tokenized_data_path, "rb") as f:
    data = pickle.load(f)

print("Tokenized data loaded!")
print(f"Keys in the dataset: {list(data.keys())}")
print(f"Number of samples: {len(data['labels'])}")

"""SPlit more

"""

from sklearn.model_selection import train_test_split

# Split data
train_input_ids, val_input_ids, train_attention_mask, val_attention_mask, train_labels, val_labels = train_test_split(
    data['input_ids'], data['attention_mask'], data['labels'], test_size=0.2, random_state=42
)

print("Data split complete:")
print(f"Training samples: {len(train_labels)}")
print(f"Validation samples: {len(val_labels)}")

import torch
from torch.utils.data import Dataset, DataLoader

class YelpDataset(Dataset):
    def __init__(self, input_ids, attention_mask, labels):
        self.input_ids = torch.tensor(input_ids)
        self.attention_mask = torch.tensor(attention_mask)
        self.labels = torch.tensor(labels)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "input_ids": self.input_ids[idx],
            "attention_mask": self.attention_mask[idx],
            "labels": self.labels[idx]
        }

# Create datasets
train_dataset = YelpDataset(train_input_ids, train_attention_mask, train_labels)
val_dataset = YelpDataset(val_input_ids, val_attention_mask, val_labels)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

print("DataLoader preparation complete!")

from transformers import BertForSequenceClassification, AdamW

# Initialize BERT model
num_labels = len(set(data['labels']))  # Number of unique labels
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Optimizer
optimizer = AdamW(model.parameters(), lr=2e-5)

# Training loop
epochs = 3
for epoch in range(epochs):
    print(f"Epoch {epoch + 1}/{epochs}")
    model.train()
    total_loss = 0

    for batch in train_loader:
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}")

import torch
import os

# Define the checkpoint saving function
def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir="/content/drive/MyDrive/checkpoints/"):
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_path = os.path.join(checkpoint_dir, f"bert_checkpoint_epoch_{epoch}.pt")
    torch.save({
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "loss": loss,
    }, checkpoint_path)
    print(f"Checkpoint saved at: {checkpoint_path}")

# Save the model's current progress
current_epoch = 2  # Replace with the current epoch
current_loss = 6.8121  # Replace with the last known loss value

save_checkpoint(model, optimizer, current_epoch, current_loss)

import torch

checkpoint_path = "/content/drive/MyDrive/checkpoints/bert_checkpoint_epoch_2.pt"

# Load the checkpoint
checkpoint = torch.load(checkpoint_path)

# Inspect the checkpoint keys
print("Keys in checkpoint:", checkpoint.keys())

import torch
from transformers import BertForSequenceClassification, AdamW

# Define the number of labels
num_labels = len(set(data['labels']))  # Assuming `data['labels']` is already loaded

# Path to the checkpoint
checkpoint_path = "/content/drive/MyDrive/checkpoints/bert_checkpoint_epoch_2.pt"

# Try loading the checkpoint
try:
    checkpoint = torch.load(checkpoint_path)
    print("Checkpoint loaded successfully!")
    start_epoch = checkpoint['epoch'] + 1
    model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer = AdamW(model.parameters(), lr=2e-5)
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    print(f"Resuming training from epoch {start_epoch}...")
except FileNotFoundError:
    print("No checkpoint found. Starting training from scratch...")
    start_epoch = 0
    model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)
    optimizer = AdamW(model.parameters(), lr=2e-5)

# Move model to device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Training loop
epochs = 3
for epoch in range(start_epoch, epochs):
    print(f"\nEpoch {epoch + 1}/{epochs}")
    model.train()
    total_loss = 0

    for batch in train_loader:
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}")

    # Save checkpoint after every epoch
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'num_labels': num_labels
    }, checkpoint_path)
    print(f"Checkpoint saved at {checkpoint_path}")

# Save the final trained model
final_model_path = "/content/drive/MyDrive/final_model"
model.save_pretrained(final_model_path)
print(f"Final model saved at {final_model_path}")

checkpoint_path = "/content/drive/MyDrive/checkpoints/bert_checkpoint_epoch_2.pt"
checkpoint = torch.load(checkpoint_path)
print(f"Epoch in checkpoint: {checkpoint['epoch']}")

num_labels = len(set(data['labels']))

import torch

# Define the checkpoint path
checkpoint_path = "/content/drive/MyDrive/checkpoints/bert_checkpoint_epoch_2.pt"

# Load the checkpoint
try:
    checkpoint = torch.load(checkpoint_path)
    print("Checkpoint loaded successfully!")

    # Check if 'num_labels' exists in the checkpoint
    if 'num_labels' in checkpoint:
        print(f"Number of labels saved in the checkpoint: {checkpoint['num_labels']}")
    else:
        print("The 'num_labels' field is not available in the checkpoint.")

    # Optionally, if you want to inspect other keys in the checkpoint
    print(f"Keys in the checkpoint: {list(checkpoint.keys())}")

except FileNotFoundError:
    print("Checkpoint not found at the specified path.")

import torch
from transformers import BertForSequenceClassification, AdamW

# Define the path to the checkpoint
checkpoint_path = "/content/drive/MyDrive/checkpoints/bert_checkpoint_epoch_2.pt"

# Infer num_labels from the dataset
num_labels = len(set(data['labels']))  # Use your dataset to infer the number of unique labels

# Load checkpoint if it exists
try:
    checkpoint = torch.load(checkpoint_path)
    print("Checkpoint loaded successfully!")

    model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer = AdamW(model.parameters(), lr=2e-5)
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
    print(f"Resuming training from epoch {start_epoch}...")
except FileNotFoundError:
    print("No checkpoint found. Starting training from scratch...")
    model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)
    optimizer = AdamW(model.parameters(), lr=2e-5)
    start_epoch = 0

# Move model to device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Training loop
epochs = 3
for epoch in range(start_epoch, epochs):
    print(f"\nEpoch {epoch + 1}/{epochs}")
    model.train()
    total_loss = 0

    for batch in train_loader:
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}")

    # Save checkpoint after every epoch
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': avg_loss
    }, checkpoint_path)
    print(f"Checkpoint saved at {checkpoint_path}")

from transformers import AutoTokenizer, BertForSequenceClassification

# Path to save the final model
save_path = "/content/drive/MyDrive/final_model"

# Reinitialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Save the model and tokenizer
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print(f"Model and tokenizer saved to {save_path}")

# Reload the model
from transformers import BertForSequenceClassification, AutoTokenizer

# Load the model and tokenizer
loaded_model = BertForSequenceClassification.from_pretrained(save_path)
loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)

# Print confirmation
print("Model and tokenizer loaded successfully!")

# Check the model's classification head to confirm correct dimensions
print(loaded_model)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
loaded_model.to(device)

def predict(texts, model, tokenizer, device):
    # Tokenize the input texts
    encodings = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=128,
        return_tensors="pt"
    )

    # Move inputs to the same device as the model
    input_ids = encodings['input_ids'].to(device)
    attention_mask = encodings['attention_mask'].to(device)

    # Set model to evaluation mode
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

    # Logits to predictions
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1).cpu().numpy()  # Move to CPU for readability
    return predictions

# Example usage
texts = ["The food was amazing!", "Terrible service."]
predictions = predict(texts, loaded_model, loaded_tokenizer, device)
print("Predictions:", predictions)

from sklearn.preprocessing import LabelEncoder

# Recreate LabelEncoder and fit it on the original categories
label_encoder = LabelEncoder()
label_encoder.fit(filtered_df['categories'])  # Replace 'filtered_df' with your dataframe variable

# Decode predictions
decoded_predictions = label_encoder.inverse_transform(predictions)
print("Decoded Predictions:", decoded_predictions)

print("All Categories:", label_encoder.classes_)



"""After this i will continue to train and make use this model for my food recommendation app!"""

